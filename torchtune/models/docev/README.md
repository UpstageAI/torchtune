# Install

ì•„ë˜ ìˆœì„œì— ë”°ë¼ í•„ìˆ˜ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:

- **PyTorch**: cu12.6 nightly ë²„ì „ ì„¤ì¹˜
  ```bash
  pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126
  ```
- **TorchTune**: ì†ŒìŠ¤ ì„¤ì¹˜ (UpstageAI/torchtune ë¦¬í¬ì§€í† ë¦¬, `docev` ë¸Œëœì¹˜)
  ```bash
  git clone --branch docev git@github.com:UpstageAI/torchtune.git  # TorchTune ì €ì¥ì†Œ í´ë¡  (docev ë¸Œëœì¹˜)
  cd torchtune
  pip install -e .
  ```

ìì„¸í•œ ì„¤ì¹˜ ê°€ì´ë“œëŠ” [TorchTune Install Guide](https://pytorch.org/torchtune/stable/install.html)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


# Training

### ğŸš€ Single-Node Training

1, 2, 4, 8 GPUì—ì„œ í•™ìŠµì„ ì‹¤í–‰í•˜ë ¤ë©´:

```bash
torchrun --nproc_per_node {1|2|4|8} \
  recipes/dev/full_finetune_distributed_ufx_dataset.py \
  --config configs/docev/docev_preview_sample.yaml
```

### ğŸ”— Distributed Multi-Node Training

`torchtune_distributed_train.sh` ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì¤‘ ë…¸ë“œ ë¶„ì‚° í•™ìŠµì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **SCRIPT_PATH**: ë¶„ì‚° í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ (`torchtune_distributed_train.sh`)
- **BASE_DIR**: TorchTune ë£¨íŠ¸ ë””ë ‰í† ë¦¬
- **CONDA_ENV_NAME**, **CONDA_ROOT**, **SSH_USER**: Conda í™˜ê²½ ë° SSH ì‚¬ìš©ì ì„¤ì •
- **NNODES**, **MASTER_NODE**, **MASTER_ADDR**, **MASTER_PORT**, **NPROC_PER_NODE**, **NODES**: ë¶„ì‚° í•™ìŠµ ë…¸ë“œ êµ¬ì„±
- **NCCL_IFACE**: NCCL í†µì‹ ì— ì‚¬ìš©í•  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤
- **CONFIG_PATH**, **SCRIPT_PATH**: í•™ìŠµ ì„¤ì • íŒŒì¼ ë° ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
- **LOG_DIR**, **RUN_NAME**: ê° ë…¸ë“œë³„ ë¡œê·¸ ë° PID íŒŒì¼ ê´€ë¦¬ ê²½ë¡œ

ìŠ¤í¬ë¦½íŠ¸ëŠ” ê° ë…¸ë“œì— SSHë¡œ ì ‘ì†í•´ `torchrun` ëª…ë ¹ì„ ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰í•˜ë©°, ë¡œê·¸ íŒŒì¼(`.log`)ê³¼ PID íŒŒì¼(`.pid`)ì„ ìƒì„±í•©ë‹ˆë‹¤. í•™ìŠµ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê±°ë‚˜ ì¢…ë£Œí•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.


# System Architecture

```text
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚ 3. Builders & Highâ€‘Level API            â”‚
                                â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                â”‚ docev_encoder_with_connector(args)      â”‚
                                â”‚   # â†’ calls: docev_vision_encoder(),    â”‚
                                â”‚            docev_ldp_v2_connector()     â”‚
                                â”‚ docev_solar_decoder(args)               â”‚
                                â”‚   # â†’ builds: FusionEmbedding,          â”‚
                                â”‚               TransformerSelfAttention  â”‚
                                â”‚ EarlyFusionModel(decoder, encoder,â€¦)    â”‚
                                â”‚   # â†’ wraps `<image>` í† í° ë¡œì§           â”‚
                                â”‚ lora_docev_* variants                   â”‚
                                â”‚   # â†’ PEFT ë°©ì‹ìœ¼ë¡œ ë¶€ë¶„ LoRA ì ìš©          â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â–²
                                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            2. Model Architecture (Core)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Vision Encoder      Connector      EncoderWithConnector           Decoder              â”‚
â”‚  (SiglipViT)         (DocEVLDPv2)   (DocEVEncoderWithConnector)   (TransformerDecoder)  â”‚
â”‚  Conv2d patch embed  MLP â†’ GELU     Vision Encoder â†’ Connector    FusionEmbedding       â”‚
â”‚  TokenPosEmbedding   avg_pool2d     â†’ Packed Sequence             â†’ Decoder layers      â”‚
â”‚  Transformers Ã— N     PEG conv                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â–²
                                          â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚             1. Data Pipeline                                    â”‚
     â”‚ Raw Sample â†’ UfxToMessages â†’ DocEVTransform â†’ Collate â†’ Inputs  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â–²
                                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  4. Utilities & Conversion (Sideâ€‘Box)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ docev_hf_to_tune(state_dict) â†” HuggingFace â†” torchtune í˜•ì‹ ë³€í™˜       â”‚
â”‚ â€¢ docev_tune_to_hf(state_dict) â†” torchtune â†” HuggingFace               â”‚
â”‚ â€¢ _utils.py: select_best_resolution(), get_padding(), unpad_image(),...â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
## 1. Data Pipeline

**ëª©í‘œ:**
- ì›ì‹œ ë°ì´í„°(í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ íŒŒì¼)ë¥¼ ëª¨ë¸ì´ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë°°ì¹˜ í…ì„œ í˜•íƒœë¡œ ë³€í™˜.

### 1.1 UfxToMessages
- **êµ¬í˜„ìœ„ì¹˜:** : `torchtune.datasets.multimodal._ufx.py`
- **ì…ë ¥:**
  - `sample` ê°ì²´: `{ "context": List[dict], "image_files": List[str] }`
- **ì£¼ìš” ì—­í• :**
  - UfxToMessages function supports two types of context format:
    - `Context format type 1`
      - Content type is string, If image is present, placeholder for image is prepended to the text
    - `Context format type 2`
      - Content type is list, If image is present, image is appended to the list
    For example:
    ```python
    context_format_type_1 = [
        {
            "role": "user",
            "content": "<image> What is shown in this image?"
        },
        {
            "role": "assistant",
            "content": "There is a red stop sign in the image."
        },
    ]

    context_format_type_2 = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": "What is shown in this image?"},
            ],
        },
        {
            "role": "assistant",
            "content": [
                {"type": "text", "text": "There is a red stop sign in the image."},
            ],
        },
    ]
    ```
    - Message object will be converted to::
    ```python
        [
            Message(
                role = "user",
                content = [
                    {"type": "image", "content": <PIL.Image.Image>},
                    {"type": "text", "content": "What is shown in this image?"},
                ],
            ),
            Message(
                role = "assistant",
                content = [
                    {"type": "text", "content": "There is a red stop sign in the image."},
                ],
            ),
            ...
        ]
    ```
- **ì¶œë ¥:**
  - `{'messages': List[Message]}` í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬.

### 1.2 DocEVTransform
- **êµ¬í˜„ìœ„ì¹˜:** : `torchtune.models.docev._transform.py`
- **ì…ë ¥:**
  - `{'messages': List[Message]}`
- **ì£¼ìš” ì—­í• :**
  - `UfxToMessages` ì¶œë ¥ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ í† í°í™” ë° ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ìˆ˜í–‰.
  1. **Tokenization**
     - `tokenizer.apply_chat_template()`ë¥¼ í†µí•´ ì‚¬ìš©ëœ ì±„íŒ… í¬ë§· ì ìš©.
     - chat_templateëŠ” [Jinja Template](https://jinja.palletsprojects.com/en/stable/templates/)ë¥¼ ë”°ë¦„.
  2. **Image Preprocessing**
     - `LlavaNextImageProcessor`ë¡œ ì¼ë§(tiles), ì •ê·œí™”(normalize).
  3. **Handling image tokens**
     - Vision Token ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  ì´ë¥¼ í† í°í™” ê²°ê³¼ì— ë°˜ì˜.
- **ì¶œë ¥ í‚¤:**
  - `tokens`, `mask`, `encoder_input.images`, `encoder_input.image_sizes`, `encoder_input.sampling_ratio`.

### 1.3 Collate ë‹¨ê³„
- **êµ¬í˜„ìœ„ì¹˜:** : `torchtune.models.docev._collate.py`
- **padded_collate_sft**
  - batch ë‚´ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒ¨ë”© ì²˜ë¦¬.
  - `List[{'tokens': List[int], 'labels': List[int]}]` â†’ `(bsz, max_len)` í˜•íƒœì˜ LongTensor
  - `padding_idx`, `ignore_idx`, `pad_to_multiple_of` ì˜µì…˜ ì§€ì›.
- **padded_collate_tiled_images_and_mask**
  - `pad_direction`: `"right"`(í›ˆë ¨), `"left"`(ì¶”ë¡ )
  - ë‚´ë¶€ì ìœ¼ë¡œ `pad_sequence` ë˜ëŠ” `left_pad_sequence` ì‚¬ìš©.
  - ì´ë¯¸ì§€: `torch.stack` ë° `torch.concat`ìœ¼ë¡œ `(bsz, n_imgs, tiles, c, h, w)` â†’ `(bsz*n_imgs, tiles*pooled_hw, dim)` í…ì„œ ìƒì„±.
  - ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ: `(bsz, n_imgs, 2)` í…ì„œ ìƒì„±.
  - Sampling ratio: `(bsz, n_imgs)` í…ì„œ ìƒì„±.

---

## 2. Model Architecture (Core)

**ëª©í‘œ:**
- ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë“ˆ êµ¬ì„±.

### 2.1 Vision Encoder
- **êµ¬í˜„ìœ„ì¹˜:** : `torchtune.models.docev._encoder.py`
- **í´ë˜ìŠ¤:** `SiglipVisionTransformer`
- **êµ¬ì„± ìš”ì†Œ:**
  1. **Conv2d patch embedding**: `in_channels â†’ embed_dim`, `kernel_size=patch_size`
  2. **TokenPosEmbedding**: íŒ¨ì¹˜ë³„ ìœ„ì¹˜ ì„ë² ë”©
  3. **TransformerSelfAttentionLayer** Ã— `num_layers`
     - `MultiHeadAttention`: Q/K/V í”„ë¡œì ì…˜, Rotary/rope ì˜µì…˜
     - MLP: `hidden_dim` â†’ `embed_dim`
  4. **CLSProjection** (ì˜µì…˜): ìµœì¢… CLS ë²¡í„° ì¶”ì¶œ

### 2.2 Connector
- **êµ¬í˜„ìœ„ì¹˜:** : `torchtune.models.docev._encoder.py`
- **í´ë˜ìŠ¤:** `DocEVLDPv2Connector`
- **ë‹¨ê³„:**
  1. **MLP projection**: `clip_embed_dim â†’ decoder_embed_dim` + GELU
  2. **Reshape**: `[bsz, imgs, tiles, num_features, dim]` â†’ `[bsz*imgs*tiles, dim, side_len, side_len]`
  3. **avg_pool2d**: `sampling_ratio`ë§Œí¼ ë‹¤ìš´ìƒ˜í”Œë§ `[bsz*imgs*tiles, dim, side_len//sampling_ratio, side_len//sampling_ratio]`
  4. **PEG conv**: ê·¸ë£¹ë³„ ì»¨ë³¼ë£¨ì…˜ì„ í†µí•´ ìœ„ì¹˜ ì •ë³´ ë³´ê°•
  5. **Flatten & Reshape**: `(bsz*imgs, tiles*pooled_hw, llm_hidden_size)`

### 2.3 DocEVEncoderWithConnector
- **êµ¬í˜„ìœ„ì¹˜:** : `torchtune.models.docev._encoder.py`
- **í´ë˜ìŠ¤:** `DocEVEncoderWithConnector`
- **ì—­í• :** Vision Encoderì™€ Connectorë¥¼ ê²°í•©í•˜ì—¬ ì´ë¯¸ì§€ í”¼ì²˜ë¥¼ LLMì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì‹œí€€ìŠ¤ë¡œ íŒ¨í‚¹
- **ë‹¨ê³„:**
  1. **Vision Encoderë¡œë¶€í„° [bsz, imgs, tiles, num_features, dim] í˜•íƒœì˜ í”¼ì²˜ íšë“**
  2. **Connector (DocEVLDPv2Connector) í˜¸ì¶œí•˜ì—¬ [bsz*imgs, tiles*pooled_hw, llm_hidden_size] ìƒì„±**
  3. **pack_image_features**:
    - base_image_featureì™€ downsampled featureë¥¼ ì¡°í•©
    - newline token ì‚½ì… (ì´ë¯¸ì§€ êµ¬ë¶„ì ì—­í• )
    - ì „ì²´ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ë¥¼ flattení•˜ì—¬ `[all_feat_len, llm_hidden_size]` ë° ê°ê°ì˜ feature_lens ë°˜í™˜

### 2.4 Decoder
- **êµ¬í˜„ìœ„ì¹˜:** : `torchtune.modules.transformer.py`
- **í´ë˜ìŠ¤:** `TransformerDecoder`
- **êµ¬ì„± ìš”ì†Œ:**
  - `FusionEmbedding`: í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ í“¨ì „ í† í° ì„ë² ë”©
  - `TransformerDecoder`:
    - `num_layers` Ã— `TransformerSelfAttentionLayer`
    - `RMSNorm` ë…¸ë§ë¼ì´ì œì´ì…˜
    - ìµœì¢… `Linear` ì¶œë ¥ â†’ ì–´íœ˜ ìˆ˜

---

## 3. Builders & Highâ€‘Level API

### 3.1 Component Builders
- **êµ¬í˜„ìœ„ì¹˜:** : `torchtune.models.docev._component_builder.py`
> **Vision Encoder Builder**
```python
docev_vision_encoder(
  tile_size: int,
  patch_size: int,
  num_layers: int,
  embed_dim: int,
  hidden_dim: int,
  num_heads: int,
  activation: Callable,
  cls_output_dim: int,
  attn_bias: bool,
  use_rope: bool,
  out_indices: Optional[List[int]],
  output_cls_projection: bool,
  max_num_tiles: int,
  in_channels: int,
  append_cls_token: bool,
) -> SiglipVisionTransformer
```
- **ì„¤ëª…:** ë¬¸ì„œ ì´ë¯¸ì§€ íƒ€ì¼ì„ íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì„ë² ë”©í•˜ê³ , ë‹¤ìˆ˜ì˜ Transformer ë ˆì´ì–´ë¡œ ì²˜ë¦¬í•˜ëŠ” ViT ê¸°ë°˜ ì¸ì½”ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- **ì£¼ìš” íŒŒë¼ë¯¸í„°:**
  - `tile_size`: ì´ë¯¸ì§€ íƒ€ì¼ í¬ê¸° ì„¤ì •
  - `patch_size`: conv2d íŒ¨ì¹˜ í¬ê¸°
  - `num_layers`: Transformer ë ˆì´ì–´ ê°œìˆ˜
  - `embed_dim`, `hidden_dim`, `num_heads`: ì„ë² ë”© ë° MLP ì°¨ì›ê³¼ ì–´í…ì…˜ í—¤ë“œ ìˆ˜
  - `use_rope`: ë¡œí„°ë¦¬ ìœ„ì¹˜ ì¸ì½”ë”© ì‚¬ìš© ì—¬ë¶€
  - `out_indices`: ì¤‘ê°„ ë ˆì´ì–´ ì¶œë ¥ ì¸ë±ìŠ¤ ì§€ì • (ë²”ìœ„ : 0 ~ `num_layers - 1`)
  - `output_cls_projection`: CLS í† í° ì¶œë ¥ í”„ë¡œì ì…˜ ì—¬ë¶€ (default: False)
  - `max_num_tiles`: ìµœëŒ€ íƒ€ì¼ ìˆ˜
  - `in_channels`: ì…ë ¥ ì´ë¯¸ì§€ ì±„ë„ ìˆ˜
  - `append_cls_token`: CLS í† í° ì¶”ê°€ ì—¬ë¶€ (default: False)

> **Connector Builder**
```python
docev_ldp_v2_connector(
  clip_embed_dim: int,
  decoder_embed_dim: int,
) -> DocEVLDPv2Connector
```
- **ì„¤ëª…:** CLIP ì¸ì½”ë”ë¡œë¶€í„° ë‚˜ì˜¨ ë¹„ì „ í”¼ì²˜ë¥¼ LLM ì…ë ¥ ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜í•œ ë’¤, `sampling_ratio` ê¸°ë°˜ ë‹¤ìš´ìƒ˜í”Œë§ê³¼ ìœ„ì¹˜ ì¸ì½”ë”© ì»¨ë³¼ë£¨ì…˜ì„ ì ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- **ì£¼ìš” íŒŒë¼ë¯¸í„°:**
  - `clip_embed_dim`: CLIP ì„ë² ë”© ì°¨ì›
  - `decoder_embed_dim`: LLM ë””ì½”ë” ì„ë² ë”© ì°¨ì›

> **Encoder with Connector**
```python
docev_encoder_with_connector(
  # Vision encoder args,
  patch_size: int,
  num_heads: int,
  clip_embed_dim: int,
  ...,
  # Connector args,
  decoder_embed_dim: int,
  connector_type: Literal["ldp_v2"],
  ...
) -> DocEVEncoderWithConnector
```
- **ì„¤ëª…:** Vision Encoderì™€ Connectorë¥¼ ê²°í•©í•˜ì—¬ í•˜ë‚˜ì˜ ì—”ì½”ë” ëª¨ë“ˆë¡œ ë¬¶ìŠµë‹ˆë‹¤. `forward()` í˜¸ì¶œ ì‹œ ì´ë¯¸ì§€ í…ì„œë¥¼ ë°›ì•„ì„œ ìµœì¢… LLM ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
- **ì¶œë ¥:** `DocEVEncoderWithConnector` ê°ì²´, `forward(images, image_sizes, sampling_ratio)`ë¡œ ì‹œí€€ìŠ¤ í”¼ì²˜ì™€ ê¸¸ì´ë¥¼ ì¶œë ¥
- ì§€ì›ê°€ëŠ¥í•œ connector íƒ€ì…ì€ í˜„ì¬ `"ldp_v2"` ë°–ì— ì—†ìŠµë‹ˆë‹¤.

> **Decoder Builder**
```python
docev_solar_decoder(
  vocab_size: int,
  fusion_vocab_size: int,
  num_layers: int,
  num_heads: int,
  num_kv_heads: int,
  embed_dim: int,
  max_seq_len: int,
  intermediate_dim: Optional[int],
  attn_dropout: float,
  norm_eps: float,
  rope_base: int,
) -> TransformerDecoder
```
- **ì„¤ëª…:** Fusion í† í° ì„ë² ë”©ê³¼ ë‹¤ìˆ˜ì˜ Transformer Decoder ë ˆì´ì–´, ìµœì¢… ì¶œë ¥ í”„ë¡œì ì…˜ì„ í¬í•¨í•˜ëŠ” ë””ì½”ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ í•¨ê»˜ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ì£¼ìš” íŒŒë¼ë¯¸í„°:**
  - `fusion_vocab_size`: ì´ë¯¸ì§€ í† í°ìš© í“¨ì „ ì–´íœ˜ í¬ê¸°. ê¸°ì¡´ vocabì—ì„œ ì œê³µí•˜ì§€ ì•ŠëŠ” tokenì„ ì¶”ê°€í•˜ì—¬ í•´ë‹¹ token embeddingë§Œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ì–´ ê¸°ì¡´ vocab_sizeê°€ 64000 ì´ê³ , multi-modal tokenì„ 64ê°œ ì¶”ê°€í•˜ë©´ ìµœì¢… vocab_sizeëŠ” 64064ê°€ ë©ë‹ˆë‹¤. ì´í›„ í•™ìŠµ configì—ì„œ `fusion_trainable = True`ë¡œ ì„¤ì •í•˜ë©´ í•´ë‹¹ token embeddingì´ í•™ìŠµë©ë‹ˆë‹¤.
  - `num_kv_heads`: í‚¤/ê°’ í—¤ë“œ ìˆ˜ ì¡°ì •(Groupedâ€‘Query Attention ì§€ì›)
  - `max_seq_len`, `rope_base`: positional embedding ë²”ìœ„ ì„¤ì •

> **LoRA Component Builders**
```python
lora_docev_ldp_v2_connector(...)
# DocEVLDPv2Connectorì— LoRA/DoRA ë ˆì´ì–´ ì¶”ê°€
lora_docev_encoder_with_connector(...)
# Encoder+Connectorì— LoRA ì ìš©
lora_docev_solar_decoder(...)
# TransformerDecoderì— LoRA ì ìš©
```
- **ì„¤ëª…:** PEFT(LoRA/DoRA) ê¸°ë²•ìœ¼ë¡œ ì–´í…ì…˜ í”„ë¡œì ì…˜ê³¼ MLPì— ì €ë­í¬ ì ì‘ ê³„ì¸µì„ ì‚½ì…í•˜ì—¬ íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹ì„ ì§€ì›í•©ë‹ˆë‹¤.

### 3.2 Model Builders
- **êµ¬í˜„ìœ„ì¹˜:** : `torchtune.models.docev._model_builder.py`
> **Transform Builder**
```python
docev_preview_transform(
  model_name_or_path: str,
  image_token: str,
  stop_tokens: List[str],
  tile_size: int,
  patch_size: int,
  max_num_tiles: int,
  min_num_tiles: int = 1,
  vision_feature_select_strategy: Literal["default","full"],
  sampling_ratio: List[int],
  apply_random_sampling_ratio: bool,
  max_seq_len: Optional[int],
  chat_template: Optional[str]
) -> DocEVTransform
```
- **ì„¤ëª…:**
  - HuggingFace í† í¬ë‚˜ì´ì €ì™€ LlavaNext ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ì´ˆê¸°í™”í•˜ì—¬, í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ í˜¼í•© ì…ë ¥ì„ ëª¨ë¸ì— ì í•©í•œ í˜•íƒœë¡œ ì „ì²˜ë¦¬í•˜ëŠ” ë³€í™˜ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
  - ì±„íŒ… í…œí”Œë¦¿, ì´ë¯¸ì§€ í† í°, ì •ì§€ í† í°(stop_tokens) ë“±ì„ ë“±ë¡í•˜ë©°, íƒ€ì¼ë§Â·ì •ê·œí™”Â·í† í°í™” ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.
- **ì…ë ¥:**
  - `model_name_or_path`: ì‚¬ì „í•™ìŠµ ëª¨ë¸ ê²½ë¡œ í˜¹ì€ HF ì‹ë³„ì
  - `image_token`: ì…ë ¥ ì‹œ ì´ë¯¸ì§€ ìœ„ì¹˜ë¥¼ í‘œì‹œí•  íŠ¹ìˆ˜ í† í°
  - `stop_tokens`: í† í°í™” ì¤‘ë‹¨ ì‹œì ì„ ì§€ì •í•˜ëŠ” í† í° ëª©ë¡
  - `vision_feature_select_strategy`: ì´ë¯¸ì§€ í”¼ì²˜ ì„ íƒ ì „ëµ
  - `sampling_ratio`: íƒ€ì¼ë§ í›„ ë‹¤ìš´ìƒ˜í”Œë§ ë¹„ìœ¨
  - `apply_random_sampling_ratio`: ëœë¤ ë‹¤ìš´ìƒ˜í”Œë§ ì ìš© ì—¬ë¶€
  - `max_seq_len`: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
  - `chat_template`: ì±„íŒ… í…œí”Œë¦¿
- **ì¶œë ¥:**
  - `DocEVTransform` ê°ì²´, `__call__(sample)`ë¡œ ë©”ì‹œì§€ë¥¼ ëª¨ë¸ ì…ë ¥(dict) í˜•íƒœë¡œ ë³€í™˜

> **Early Fusion Model Builder**
```python
docev_preview(
  image_token_id: int,
  decoder_trainable: bool,
  encoder_trainable: bool | Dict[str,bool],
  fusion_trainable: bool
) -> EarlyFusionModel
```
- **ì„¤ëª…:**
  - ì»´í¬ë„ŒíŠ¸ ë¹Œë”ì—ì„œ ìƒì„±ëœ `DocEVEncoderWithConnector`ì™€ `TransformerDecoder`ë¥¼ ê²°í•©í•˜ì—¬, `<image>` í† í° ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ ì¸í¼ëŸ°ìŠ¤/í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ìµœì¢… ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
  - `decoder_trainable`, `encoder_trainable`, `fusion_trainable` í”Œë˜ê·¸ë¡œ ê° íŒŒíŠ¸ì˜ í•™ìŠµ ê°€ëŠ¥ ì—¬ë¶€ë¥¼ ì œì–´í•©ë‹ˆë‹¤.
- **ì…ë ¥:**
  - `image_token_id`: í† í¬ë‚˜ì´ì €ì—ì„œ ë¶€ì—¬í•œ ì´ë¯¸ì§€ í† í° ID
  - `decoder_trainable`: ë””ì½”ë” íŒŒë¼ë¯¸í„° í•™ìŠµ ì—¬ë¶€
  - `encoder_trainable`: ì¸ì½”ë” íŒŒë¼ë¯¸í„° í•™ìŠµ ì—¬ë¶€ (True/False ë˜ëŠ” ì„¸ë¶€ dict)
  - `fusion_trainable`: í“¨ì „(ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ì„ë² ë”©) íŒŒë¼ë¯¸í„° í•™ìŠµ ì—¬ë¶€
- **ì¶œë ¥:**
  - `EarlyFusionModel` ê°ì²´, `forward(tokens=None, encoder_input=...)`ë¡œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬

> **LoRA Model Builder**
```python
lora_docev_preview(
  lora_attn_modules: List[LORA_ATTN_MODULES],
  image_token_id: int,
  decoder_trainable: str,
  encoder_trainable: str,
  fusion_trainable: str,
  apply_lora_to_mlp: bool,
  apply_lora_to_output: bool,
  lora_rank: int,
  lora_alpha: int,
  lora_dropout: float,
  use_dora: bool,
  quantize_base: bool
) -> EarlyFusionModel
```
- **ì„¤ëª…:**
  - `docev_preview`ì—ì„œ ìƒì„±ëœ EarlyFusionModelì— PEFT(LoRA/DoRA)ë¥¼ ì ìš©í•˜ì—¬, ì–´í…ì…˜ ë° MLP ë ˆì´ì–´ì— ê²½ëŸ‰ ì ì‘ ê³„ì¸µì„ ì¶”ê°€í•œ ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
  - `decoder_trainable`, `encoder_trainable`, `fusion_trainable`ì€ "full", "lora", "frozen" ì˜µì…˜ì„ ì§€ì›í•˜ì—¬ ì„¸ë¶€ ì œì–´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- **ì¶œë ¥:**
  - LoRAê°€ ì ìš©ëœ `EarlyFusionModel` ê°ì²´

> **QLoRA Shortcut**
```python
qlora_docev_preview = partial(
  lora_docev_preview,
  quantize_base=True
)
```
- **ì„¤ëª…:**
  - `quantize_base=True`ë¡œ ì„¤ì •ëœ QLoRA êµ¬ì„±ì„ ë¯¸ë¦¬ ì§€ì •í•œ í¸ì˜ í•¨ìˆ˜ë¡œ, 4-bit ì–‘ìí™”ë¥¼ ë™ë°˜í•œ LoRA ëª¨ë¸ì„ ì†ì‰½ê²Œ ìƒì„±í•©ë‹ˆë‹¤ã€‚


---

## 4. Utilities & Conversion (Sideâ€‘Box)

**ëª©í‘œ:**
- ì™¸ë¶€ í¬ë§· â†” ë‚´ë¶€ í¬ë§· ë³€í™˜ ë° ë¶€ê°€ì ì¸ í—¬í¼ í•¨ìˆ˜

- **State dict ë³€í™˜:**
  - `docev_hf_to_tune(state_dict)`: HuggingFace â†’ torchtune ë„¤ì´ë°/í¬ë§·
  - `docev_tune_to_hf(state_dict)`: torchtune â†’ HuggingFace
- **_utils.py**:
  - `select_best_resolution()`: ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• í›„ ìµœì  ê·¸ë¦¬ë“œ ì„ íƒ
  - `get_padding()`: image ì‹¤ì œ ì‚¬ì´ì¦ˆë¥¼ ê³ ë ¤í•˜ì—¬ padding í¬ê¸° ê³„ì‚°
  - `unpad_image()`: ì‹¤ì œ image sizeë¥¼ ê³ ë ¤í•˜ì—¬ connector outputì—ì„œ padding ì œê±°
  - `get_encoder_output_feature_size()`: ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆì— ë”°ë¥¸ í”¼ì²˜ ìˆ˜ ê³„ì‚°
- **_transform.py**:
  - `UfxToMessages`: Message ê°ì²´ ë³€í™˜ ë¡œì§

---
